{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Vektonn is a high-performance battle-tested kNN vector search engine for your data science applications. It helps you manage vectors' lifecycle and radically reduces time to market. Vektonn has the following features: Support for both dense and sparse vectors Precise and approximate kNN (AkNN) algorithms Scalable architecture that allows to easily handle hundreds-of-GB-worth of vector data and many more . Components \u00b6 There are three main parts of Vektonn: an API, an Index, and a Data Source. The API has methods for search and uploading vector data. It proxies requests to corresponding Indices and Data Sources. A Data Source is where all the vectors' data being persistently stored. Currently, a Data Source is implemented using Apache Kafka . An Index is an in-memory snapshot of data in Data Source. It updates asynchronously from a corresponding Data Source. A data from a single Data Source can be spread (sharded) over several Indices to fit in RAM of hosting nodes. A single Data Source may have several Indices defined on it with different metrics. GitHub repositories \u00b6 vektonn is a main repository with implementations for the API, Index shards and Data Sources. It also contains sources for Vektonn's .NET client . vektonn-index is a .NET library for finding nearest neighbors in vector space. It provides an implementation of basic functionality for Indices: vector storage and retrieval. vektonn-client-python hosts Python client for Vektonn source code. vectonn-examples is a repository with various examples of Vektonn usage. Support \u00b6 If you have any questions or need help with Vektonn please contact us on Slack channel . License \u00b6 Vektonn is licensed under Apache License 2.0 , so you may freely use it for commercial purposes.","title":"Overview"},{"location":"#overview","text":"Vektonn is a high-performance battle-tested kNN vector search engine for your data science applications. It helps you manage vectors' lifecycle and radically reduces time to market. Vektonn has the following features: Support for both dense and sparse vectors Precise and approximate kNN (AkNN) algorithms Scalable architecture that allows to easily handle hundreds-of-GB-worth of vector data and many more .","title":"Overview"},{"location":"#components","text":"There are three main parts of Vektonn: an API, an Index, and a Data Source. The API has methods for search and uploading vector data. It proxies requests to corresponding Indices and Data Sources. A Data Source is where all the vectors' data being persistently stored. Currently, a Data Source is implemented using Apache Kafka . An Index is an in-memory snapshot of data in Data Source. It updates asynchronously from a corresponding Data Source. A data from a single Data Source can be spread (sharded) over several Indices to fit in RAM of hosting nodes. A single Data Source may have several Indices defined on it with different metrics.","title":"Components"},{"location":"#github-repositories","text":"vektonn is a main repository with implementations for the API, Index shards and Data Sources. It also contains sources for Vektonn's .NET client . vektonn-index is a .NET library for finding nearest neighbors in vector space. It provides an implementation of basic functionality for Indices: vector storage and retrieval. vektonn-client-python hosts Python client for Vektonn source code. vectonn-examples is a repository with various examples of Vektonn usage.","title":"GitHub repositories"},{"location":"#support","text":"If you have any questions or need help with Vektonn please contact us on Slack channel .","title":"Support"},{"location":"#license","text":"Vektonn is licensed under Apache License 2.0 , so you may freely use it for commercial purposes.","title":"License"},{"location":"features/","text":"Features \u00b6 Ease of use \u00b6 Storing metadata (attributes) \u00b6 We store not only embeddings/vectors themselves, but also their metadata (attributes), which allows you to use real-world entities from your domain. For example, attributes can be a real-world identifiers (such as name) or really any custom data you want to store along with vectors. Dense and sparse vector support \u00b6 You can work with vectors of any type \u2014 dense or sparse. For example, to solve word processing problems, you can use bag-of-words and load appropriate sparse vectors into Vektonn. All set and ready to go \u00b6 We have SDKs for Python and for .NET along with published Docker images on DockerHub \u2014 everything you'll need for a quick start . Performance and scalability \u00b6 Low overhead \u00b6 We have a very thin and efficient management layer atop of actual binary indices, so the overhead is pretty low. Sharding \u00b6 For horizontal scaling, you can specify the attributes by which the vectors will be distributed into groups (or index shards). When processing a search query, the results from multiple shards will be automatically combined. Data filtering (splitting) \u00b6 Each shard can be further split into logical parts for an even more efficient search. Just specify split attributes in the indexing scheme, and all the queries for that index will filter out unnecessary data before searching. For example, you may efficiently search for some goods in a particular store, or for books written in a specific language. Data lifecycle management \u00b6 Online changes \u00b6 We support changing indices as new data arrives (delete, update, or insert data to the index), concurrently with search queries. Seamless versioning \u00b6 You can deploy multiple indices over a single data source (containing the same vectors and attributes) and seamlessly transition to their new versions. Different indices may have different configuration parameters.","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"features/#ease-of-use","text":"","title":"Ease of use"},{"location":"features/#performance-and-scalability","text":"","title":"Performance and scalability"},{"location":"features/#data-lifecycle-management","text":"","title":"Data lifecycle management"},{"location":"quick-start/","text":"Quick Start \u00b6 Run Vektonn \u00b6 In order to run Vektonn locally you will need to install Docker . Run Vektonn configured for QuickStart examples: 1 2 3 git clone https://github.com/vektonn/vektonn-examples.git /path/to/vektonn-examples /path/to/vektonn-examples/docker/run-vektonn.sh QuickStart.Index Look at docker-compose.yaml to understand how to setup local Vektonn with single Index shard. Docker images for Vektonn are published on Docker Hub . Use Vektonn API with a Python client \u00b6 You will need Python >= 3.7 to run this sample. Install Vektonn SDK for Python: 1 pip install vektonn Initialize Vektonn client: 1 2 3 from vektonn import Vektonn vektonn_client = Vektonn ( 'http://localhost:8081' ) Upload data to Vektonn: 1 2 3 4 5 6 7 8 9 10 11 12 13 from vektonn.dtos import Attribute , AttributeValue , InputDataPoint , Vector vektonn_client . upload ( data_source_name = 'QuickStart.Source' , data_source_version = '1.0' , input_data_points = [ InputDataPoint ( attributes = [ Attribute ( key = 'id' , value = AttributeValue ( int64 = 1 )), Attribute ( key = 'payload' , value = AttributeValue ( string = 'sample data point' )), ], vector = Vector ( is_sparse = False , coordinates = [ 3.14 , 2.71 ])) ]) Search for k nearest data points to the given query_vector : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from vektonn.dtos import Vector , SearchQuery k = 10 query_vector = Vector ( is_sparse = False , coordinates = [ 1.2 , 3.4 ]) search_results = vektonn_client . search ( index_name = 'QuickStart.Index' , index_version = '1.0' , search_query = SearchQuery ( k = k , query_vectors = [ query_vector ])) print ( f 'For query vector { query_vector . coordinates } { k } nearest data points are:' ) for fdp in search_results [ 0 ] . nearest_data_points : attrs = { x . key : x . value for x in fdp . attributes } distance , vector , dp_id , payload = fdp . distance , fdp . vector , attrs [ 'id' ] . int64 , attrs [ 'payload' ] . string print ( f ' - \" { payload } \" with id = { dp_id } , vector = { vector . coordinates } , distance = { distance } ' ) What's next \u00b6 Take a look at Jupyter notebooks with several examples of solving problems similar to real ones using vector spaces and Vektonn: Hotels . Task: save the user's time while searching for hotels. This example uses vectorized user reviews to find hotels. Price Match Guarantee . Task: help the seller establish a competitive price for their product on the marketplace. Books . Task: find similar books by user reviews. This example demonstrates usage of sparse vectors.","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"","title":"Quick Start"},{"location":"quick-start/#run-vektonn","text":"In order to run Vektonn locally you will need to install Docker . Run Vektonn configured for QuickStart examples: 1 2 3 git clone https://github.com/vektonn/vektonn-examples.git /path/to/vektonn-examples /path/to/vektonn-examples/docker/run-vektonn.sh QuickStart.Index Look at docker-compose.yaml to understand how to setup local Vektonn with single Index shard. Docker images for Vektonn are published on Docker Hub .","title":"Run Vektonn"},{"location":"quick-start/#use-vektonn-api-with-a-python-client","text":"You will need Python >= 3.7 to run this sample. Install Vektonn SDK for Python: 1 pip install vektonn Initialize Vektonn client: 1 2 3 from vektonn import Vektonn vektonn_client = Vektonn ( 'http://localhost:8081' ) Upload data to Vektonn: 1 2 3 4 5 6 7 8 9 10 11 12 13 from vektonn.dtos import Attribute , AttributeValue , InputDataPoint , Vector vektonn_client . upload ( data_source_name = 'QuickStart.Source' , data_source_version = '1.0' , input_data_points = [ InputDataPoint ( attributes = [ Attribute ( key = 'id' , value = AttributeValue ( int64 = 1 )), Attribute ( key = 'payload' , value = AttributeValue ( string = 'sample data point' )), ], vector = Vector ( is_sparse = False , coordinates = [ 3.14 , 2.71 ])) ]) Search for k nearest data points to the given query_vector : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from vektonn.dtos import Vector , SearchQuery k = 10 query_vector = Vector ( is_sparse = False , coordinates = [ 1.2 , 3.4 ]) search_results = vektonn_client . search ( index_name = 'QuickStart.Index' , index_version = '1.0' , search_query = SearchQuery ( k = k , query_vectors = [ query_vector ])) print ( f 'For query vector { query_vector . coordinates } { k } nearest data points are:' ) for fdp in search_results [ 0 ] . nearest_data_points : attrs = { x . key : x . value for x in fdp . attributes } distance , vector , dp_id , payload = fdp . distance , fdp . vector , attrs [ 'id' ] . int64 , attrs [ 'payload' ] . string print ( f ' - \" { payload } \" with id = { dp_id } , vector = { vector . coordinates } , distance = { distance } ' )","title":"Use Vektonn API with a Python client"},{"location":"quick-start/#whats-next","text":"Take a look at Jupyter notebooks with several examples of solving problems similar to real ones using vector spaces and Vektonn: Hotels . Task: save the user's time while searching for hotels. This example uses vectorized user reviews to find hotels. Price Match Guarantee . Task: help the seller establish a competitive price for their product on the marketplace. Books . Task: find similar books by user reviews. This example demonstrates usage of sparse vectors.","title":"What's next"},{"location":"reference/api/","text":"Python client \u00b6 To install Vektonn client: 1 pip install vektonn See Python Quick start for example usage. The source code is available in the vektonn-client-python repository . .NET client \u00b6 To add Vektonn to your project: 1 dotnet add package Vektonn.ApiClient See .NET Quick start for example usage. The source code is available in the main repository . HTTP API \u00b6 For all other languages you can use Vektonn's HTTP API directly. See Vektonn examples for some examples.","title":"API"},{"location":"reference/api/#python-client","text":"To install Vektonn client: 1 pip install vektonn See Python Quick start for example usage. The source code is available in the vektonn-client-python repository .","title":"Python client"},{"location":"reference/api/#net-client","text":"To add Vektonn to your project: 1 dotnet add package Vektonn.ApiClient See .NET Quick start for example usage. The source code is available in the main repository .","title":".NET client"},{"location":"reference/api/#http-api","text":"For all other languages you can use Vektonn's HTTP API directly. See Vektonn examples for some examples.","title":"HTTP API"},{"location":"reference/specification/","text":"Data points \u00b6 Data point is a single piece of data to store in Vektonn. It contains both vector (dense or sparse) and its attributes. You can use attributes to store relevant data from your domain. 1 2 3 4 5 6 7 8 9 10 DataPoint := ( Vector , Attributes ) Vector := DenseVector | SparseVector DenseVector := ( coordinates : list [ float64 ]) SparseVector := ( coordinates : list [ float64 ], coordinateIndices : list [ int ]) Attribute := ( key : AttributeKey , value : AttributeValue ) Attributes := dict [ AttributeKey , AttributeValue ] AttributeKey := str AttributeValue := bool | int64 | float64 | str | UUID | datetime Data sources \u00b6 Data source is a named (and versioned) persistent storage for data points. It represents a timeline of all data points' updates (uploads and deletions). 1 2 3 4 5 6 7 8 9 10 11 DataSourceId := ( name : str , version : str ) DataSourceMeta := ( id : DataSourceId , vectorDimension : int , vectorsAreSparse : bool , permanentAttributes : set [ AttributeKey ], attributeValueTypes : dict [ AttributeKey , AttributeValueType ] # defines a type of AttributeValue for each AttributeKey ) Defining a data source requires you to set the type of data points that it will store, including their vector dimension and permanent attributes. Permanent attributes are attributes that all data points must have. Neither the set of keys nor the values of permanentAttributes can change for a given data point over time. You cannot query a data source directly \u2014 you have to create an index for that. Indices \u00b6 Index is a snapshot of a single data source that allows to query for data points. Indices are asynchronously updated, making them eventually consistent with their data sources. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 IndexAlgorithm := ( type : str , params : Optional [ dict [ str , str ]]) # (1) IndexMeta := ( id : ( name : str , version : str ), dataSource : DataSourceId , indexAlgorithm : IndexAlgorithm , idAttributes : set [ AttributeKey ], splitAttributes : set [ AttributeKey ], shardAttributes : set [ AttributeKey ], payloadAttributes : set [ AttributeKey ] ) Index . idAttributes \u2260 \u2205 Index . permanentAttributes := Index . idAttributes \u22c3 Index . splitAttributes \u22c3 Index . shardAttributes Index . permanentAttributes \u2286 DataSourceMeta . permanentAttributes Index . permanentAttributes \u22c2 Index . payloadAttributes \u2261 \u2205 See the list of supported algorithms . Index's permanent attributes are: idAttributes \u2014 a set of keys from your domain that uniquely identifies an object represented by the vector; shardAttributes \u2014 defines a sharding scheme; splitAttributes \u2014 defines a splitting scheme. Note that idAttributes , splitAttributes , and shardAttributes sets can be in arbitrary relationships, but each one must be a subset of a corresponding data source's permanent attributes. They cannot be changed during vector's lifetime. All other attributes are payloadAttributes \u2014 any metadata to be stored alongside the vector that may be freely updated. Sharding \u00b6 Shards are a physical representation of an index. Each shard has a subset of index's data according to defined sharding scheme. It's possible to have a single shard for an index: that way everything is located on a single machine. Multiple shards, however, are useful to distribute data across several nodes to fit into RAM limitations. Splitting \u00b6 Splitting is an optional scheme of dividing shard's data within a single physical node. Splits are useful to speed up search: it uses data only from requested splits, everything else is ignored. Your domain's data may be naturally divided: for example, when you always perform queries using the data within geographical regions. To make use of index's splitting, you have to provide values for every corresponding attribute from a splitting scheme on each search upon that index. Otherwise, the search will be conducted on all partially matched splits and merged afterwards. You may have several indices running upon the same data source \u2014 with different splitting schemes (including no splitting at all). Updating data \u00b6 1 2 3 4 5 Upload := Callable [[ UploadQuery ], None ] UploadQuery := list [ InputDataPoint ] InputDataPoint := DataPoint | Tombstone Tombstone := ( permanentAttributes : Attributes ) # uniquely identify a data point to delete Data updates are one of either: an upload of a new data point; an update of existing data point identified by its permanent attributes; a deletion of a previously uploaded data point, marked by a tombstone . Searching \u00b6 Search for k nearest neighbors for each of queryVectors : 1 2 3 4 5 6 Search := Callable [[ SearchQuery ], list [ SearchResult ]] SearchQuery := ( k : int , queryVectors : list [ Vector ], splitFilter : Optional [ Attributes ]) SearchResult := ( queryVector : Vector , nearestDataPoints : list [ FoundDataPoint ]) FoundDataPoint := ( dataPoint : DataPoint , distance : float64 ) If SearchQuery.splitFilter is present then SearchQuery.splitFilter.keys \u2286 IndexMeta.splitAttributes .","title":"Sort of formal specification"},{"location":"reference/specification/#data-points","text":"Data point is a single piece of data to store in Vektonn. It contains both vector (dense or sparse) and its attributes. You can use attributes to store relevant data from your domain. 1 2 3 4 5 6 7 8 9 10 DataPoint := ( Vector , Attributes ) Vector := DenseVector | SparseVector DenseVector := ( coordinates : list [ float64 ]) SparseVector := ( coordinates : list [ float64 ], coordinateIndices : list [ int ]) Attribute := ( key : AttributeKey , value : AttributeValue ) Attributes := dict [ AttributeKey , AttributeValue ] AttributeKey := str AttributeValue := bool | int64 | float64 | str | UUID | datetime","title":"Data points"},{"location":"reference/specification/#data-sources","text":"Data source is a named (and versioned) persistent storage for data points. It represents a timeline of all data points' updates (uploads and deletions). 1 2 3 4 5 6 7 8 9 10 11 DataSourceId := ( name : str , version : str ) DataSourceMeta := ( id : DataSourceId , vectorDimension : int , vectorsAreSparse : bool , permanentAttributes : set [ AttributeKey ], attributeValueTypes : dict [ AttributeKey , AttributeValueType ] # defines a type of AttributeValue for each AttributeKey ) Defining a data source requires you to set the type of data points that it will store, including their vector dimension and permanent attributes. Permanent attributes are attributes that all data points must have. Neither the set of keys nor the values of permanentAttributes can change for a given data point over time. You cannot query a data source directly \u2014 you have to create an index for that.","title":"Data sources"},{"location":"reference/specification/#indices","text":"Index is a snapshot of a single data source that allows to query for data points. Indices are asynchronously updated, making them eventually consistent with their data sources. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 IndexAlgorithm := ( type : str , params : Optional [ dict [ str , str ]]) # (1) IndexMeta := ( id : ( name : str , version : str ), dataSource : DataSourceId , indexAlgorithm : IndexAlgorithm , idAttributes : set [ AttributeKey ], splitAttributes : set [ AttributeKey ], shardAttributes : set [ AttributeKey ], payloadAttributes : set [ AttributeKey ] ) Index . idAttributes \u2260 \u2205 Index . permanentAttributes := Index . idAttributes \u22c3 Index . splitAttributes \u22c3 Index . shardAttributes Index . permanentAttributes \u2286 DataSourceMeta . permanentAttributes Index . permanentAttributes \u22c2 Index . payloadAttributes \u2261 \u2205 See the list of supported algorithms . Index's permanent attributes are: idAttributes \u2014 a set of keys from your domain that uniquely identifies an object represented by the vector; shardAttributes \u2014 defines a sharding scheme; splitAttributes \u2014 defines a splitting scheme. Note that idAttributes , splitAttributes , and shardAttributes sets can be in arbitrary relationships, but each one must be a subset of a corresponding data source's permanent attributes. They cannot be changed during vector's lifetime. All other attributes are payloadAttributes \u2014 any metadata to be stored alongside the vector that may be freely updated.","title":"Indices"},{"location":"reference/specification/#sharding","text":"Shards are a physical representation of an index. Each shard has a subset of index's data according to defined sharding scheme. It's possible to have a single shard for an index: that way everything is located on a single machine. Multiple shards, however, are useful to distribute data across several nodes to fit into RAM limitations.","title":"Sharding"},{"location":"reference/specification/#splitting","text":"Splitting is an optional scheme of dividing shard's data within a single physical node. Splits are useful to speed up search: it uses data only from requested splits, everything else is ignored. Your domain's data may be naturally divided: for example, when you always perform queries using the data within geographical regions. To make use of index's splitting, you have to provide values for every corresponding attribute from a splitting scheme on each search upon that index. Otherwise, the search will be conducted on all partially matched splits and merged afterwards. You may have several indices running upon the same data source \u2014 with different splitting schemes (including no splitting at all).","title":"Splitting"},{"location":"reference/specification/#updating-data","text":"1 2 3 4 5 Upload := Callable [[ UploadQuery ], None ] UploadQuery := list [ InputDataPoint ] InputDataPoint := DataPoint | Tombstone Tombstone := ( permanentAttributes : Attributes ) # uniquely identify a data point to delete Data updates are one of either: an upload of a new data point; an update of existing data point identified by its permanent attributes; a deletion of a previously uploaded data point, marked by a tombstone .","title":"Updating data"},{"location":"reference/specification/#searching","text":"Search for k nearest neighbors for each of queryVectors : 1 2 3 4 5 6 Search := Callable [[ SearchQuery ], list [ SearchResult ]] SearchQuery := ( k : int , queryVectors : list [ Vector ], splitFilter : Optional [ Attributes ]) SearchResult := ( queryVector : Vector , nearestDataPoints : list [ FoundDataPoint ]) FoundDataPoint := ( dataPoint : DataPoint , distance : float64 ) If SearchQuery.splitFilter is present then SearchQuery.splitFilter.keys \u2286 IndexMeta.splitAttributes .","title":"Searching"},{"location":"reference/supported-algorithms/","text":"For dense vectors \u00b6 These algorithms are based on Faiss library : FaissIndex.L2 \u2014 squared Euclidean (L2) distance. FaissIndex.IP \u2014 this is typically used for maximum inner product search. This is not by itself cosine similarity, unless the vectors are normalized. By default FaissIndex -es are constructed in Flat mode, i.e. they implement exhaustive (precise) search. To use Faiss implementation of HNSW index provide Hnsw_M , Hnsw_EfConstruction , and Hnsw_EfSearch parameters. For sparse vectors \u00b6 These algorithms are derived from PySparNN library : SparnnIndex.Cosine \u2014 Cosine distance (i.e. 1 - cosine_similarity ). SparnnIndex.JaccardBinary \u2014 Jaccard distance for binary vectors (i.e. vectors whose coordinates have the values 0 or 1).","title":"Supported index algorithms"},{"location":"reference/supported-algorithms/#for-dense-vectors","text":"These algorithms are based on Faiss library : FaissIndex.L2 \u2014 squared Euclidean (L2) distance. FaissIndex.IP \u2014 this is typically used for maximum inner product search. This is not by itself cosine similarity, unless the vectors are normalized. By default FaissIndex -es are constructed in Flat mode, i.e. they implement exhaustive (precise) search. To use Faiss implementation of HNSW index provide Hnsw_M , Hnsw_EfConstruction , and Hnsw_EfSearch parameters.","title":"For dense vectors"},{"location":"reference/supported-algorithms/#for-sparse-vectors","text":"These algorithms are derived from PySparNN library : SparnnIndex.Cosine \u2014 Cosine distance (i.e. 1 - cosine_similarity ). SparnnIndex.JaccardBinary \u2014 Jaccard distance for binary vectors (i.e. vectors whose coordinates have the values 0 or 1).","title":"For sparse vectors"}]}